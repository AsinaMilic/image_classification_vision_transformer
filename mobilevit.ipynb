{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmT_58DM2qIl"
      },
      "source": [
        "# MobileViT: A mobile-friendly Transformer-based model for image classification\n",
        "\n",
        "\n",
        "**Description:** MobileViT for image classification with combined benefits of convolutions and Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Wa6ip62qIp"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we implement the MobileViT architecture\n",
        "([Mehta et al.](https://arxiv.org/abs/2110.02178)),\n",
        "which combines the benefits of Transformers\n",
        "([Vaswani et al.](https://arxiv.org/abs/1706.03762))\n",
        "and convolutions. With Transformers, we can capture long-range dependencies that result\n",
        "in global representations. With convolutions, we can capture spatial relationships that\n",
        "model locality.\n",
        "\n",
        "Besides combining the properties of Transformers and convolutions, the authors introduce\n",
        "MobileViT as a general-purpose mobile-friendly backbone for different image recognition\n",
        "tasks. Their findings suggest that, performance-wise, MobileViT is better than other\n",
        "models with the same or higher complexity ([MobileNetV3](https://arxiv.org/abs/1905.02244),\n",
        "for example), while being efficient on mobile devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3qdz-If2qIr"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install tensorflow==2.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6mLuacA2t2T",
        "outputId": "0a783ddd-3c9c-4d47-875f-05d1fe74111c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: tensorflow==2.8.0 in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (23.5.26)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.32.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.56.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.41.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uLTO8NAx2qIt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.applications import imagenet_utils\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "#import tensorflow_addons as tfa\n",
        "\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-i8Hdvc2qIu"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "obpjTt812qIw"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "patch_size = 4 # 2x2, for the Transformer blocks.\n",
        "image_size = 512 # 256\n",
        "expansion_factor = 2\n",
        "batch_size = 4 # 8 na Chiro#16 i 32 i 64 puca na Chiro smaller\n",
        "auto = tf.data.AUTOTUNE\n",
        "resize_bigger = 560\n",
        "num_classes = 10\n",
        "epochs = 30\n",
        "learning_rate = 0.002\n",
        "label_smoothing_factor = 0.1\n",
        "#train_dir = '/content/drive/MyDrive/Chiro10_Smaller/train_512'\n",
        "#test_dir = '/content/drive/MyDrive/Chiro10_Smaller/test_512'\n",
        "#val_dir = '/content/drive/MyDrive/Chiro10_Smaller/val_512'\n",
        "train_dir = '/content/drive/MyDrive/Chiro10/train_512'\n",
        "test_dir = '/content/drive/MyDrive/Chiro10/test_512'\n",
        "val_dir = '/content/drive/MyDrive/Chiro10/val_512'\n",
        "checkpoint_filepath = \"/tmp/checkpoint\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEBI3adz2qIw"
      },
      "source": [
        "## MobileViT utilities\n",
        "\n",
        "The MobileViT architecture is comprised of the following blocks:\n",
        "\n",
        "* Strided 3x3 convolutions that process the input image.\n",
        "* [MobileNetV2](https://arxiv.org/abs/1801.04381)-style inverted residual blocks for\n",
        "downsampling the resolution of the intermediate feature maps.\n",
        "* MobileViT blocks that combine the benefits of Transformers and convolutions. It is\n",
        "presented in the figure below (taken from the\n",
        "[original paper](https://arxiv.org/abs/2110.02178)):\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/mANnhI7.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GG3pKhP12qIx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def conv_block(x, filters=16, kernel_size=3, strides=2):\n",
        "    conv_layer = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, activation=tf.nn.swish, padding=\"same\"\n",
        "    )\n",
        "    return conv_layer(x)\n",
        "\n",
        "\n",
        "# Reference: https://git.io/JKgtC\n",
        "\n",
        "\n",
        "def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
        "    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    if strides == 2:\n",
        "        m = layers.ZeroPadding2D(padding=imagenet_utils.correct_pad(m, 3))(m)\n",
        "    m = layers.DepthwiseConv2D(\n",
        "        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n",
        "    )(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "\n",
        "    if tf.math.equal(x.shape[-1], output_channels) and strides == 1:\n",
        "        return layers.Add()([m, x])\n",
        "    return m\n",
        "\n",
        "\n",
        "# Reference:\n",
        "# https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
        "\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.swish)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=[x.shape[-1] * 2, x.shape[-1]], dropout_rate=0.1,)\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n",
        "    # Local projection with convolutions.\n",
        "    local_features = conv_block(x, filters=projection_dim, strides=strides)\n",
        "    local_features = conv_block(\n",
        "        local_features, filters=projection_dim, kernel_size=1, strides=strides\n",
        "    )\n",
        "\n",
        "    # Unfold into patches and then pass through Transformers.\n",
        "    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n",
        "    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n",
        "        local_features\n",
        "    )\n",
        "    global_features = transformer_block(\n",
        "        non_overlapping_patches, num_blocks, projection_dim\n",
        "    )\n",
        "\n",
        "    # Fold into conv-like feature-maps.\n",
        "    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n",
        "        global_features\n",
        "    )\n",
        "\n",
        "    # Apply point-wise conv -> concatenate with the input features.\n",
        "    folded_feature_map = conv_block(\n",
        "        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n",
        "\n",
        "    # Fuse the local and global features using a convoluion layer.\n",
        "    local_global_features = conv_block(\n",
        "        local_global_features, filters=projection_dim, strides=strides\n",
        "    )\n",
        "\n",
        "    return local_global_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWCtnAiB2qI0"
      },
      "source": [
        "**More on the MobileViT block**:\n",
        "\n",
        "* First, the feature representations (A) go through convolution blocks that capture local\n",
        "relationships. The expected shape of a single entry here would be `(h, w, num_channels)`.\n",
        "* Then they get unfolded into another vector with shape `(p, n, num_channels)`,\n",
        "where `p` is the area of a small patch, and `n` is `(h * w) / p`. So, we end up with `n`\n",
        "non-overlapping patches.\n",
        "* This unfolded vector is then passed through a Tranformer block that captures global\n",
        "relationships between the patches.\n",
        "* The output vector (B) is again folded into a vector of shape `(h, w, num_channels)`\n",
        "resembling a feature map coming out of convolutions.\n",
        "\n",
        "Vectors A and B are then passed through two more convolutional layers to fuse the local\n",
        "and global representations. Notice how the spatial resolution of the final vector remains\n",
        "unchanged at this point. The authors also present an explanation of how the MobileViT\n",
        "block resembles a convolution block of a CNN. For more details, please refer to the\n",
        "original paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkSE-Vcb2qI3"
      },
      "source": [
        "Next, we combine these blocks together and implement the MobileViT architecture (XXS\n",
        "variant). The following figure (taken from the original paper) presents a schematic\n",
        "representation of the architecture:\n",
        "\n",
        "![](https://i.ibb.co/sRbVRBN/image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G63R9YOh2qI3",
        "outputId": "658d969f-518a-4ce6-ed6f-f49e75c0e830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(None, 512, 512, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " rescaling_5 (Rescaling)        (None, 512, 512, 3)  0           ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_140 (Conv2D)            (None, 256, 256, 16  448         ['rescaling_5[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_141 (Conv2D)            (None, 256, 256, 32  512         ['conv2d_140[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_105 (Batch  (None, 256, 256, 32  128        ['conv2d_141[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.silu_70 (TFOpLambda)     (None, 256, 256, 32  0           ['batch_normalization_105[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " depthwise_conv2d_35 (Depthwise  (None, 256, 256, 32  288        ['tf.nn.silu_70[0][0]']          \n",
            " Conv2D)                        )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_106 (Batch  (None, 256, 256, 32  128        ['depthwise_conv2d_35[0][0]']    \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.silu_71 (TFOpLambda)     (None, 256, 256, 32  0           ['batch_normalization_106[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_142 (Conv2D)            (None, 256, 256, 16  512         ['tf.nn.silu_71[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_107 (Batch  (None, 256, 256, 16  64         ['conv2d_142[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_105 (Add)                  (None, 256, 256, 16  0           ['batch_normalization_107[0][0]',\n",
            "                                )                                 'conv2d_140[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_143 (Conv2D)            (None, 256, 256, 32  512         ['add_105[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_108 (Batch  (None, 256, 256, 32  128        ['conv2d_143[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.silu_72 (TFOpLambda)     (None, 256, 256, 32  0           ['batch_normalization_108[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " zero_padding2d_20 (ZeroPadding  (None, 257, 257, 32  0          ['tf.nn.silu_72[0][0]']          \n",
            " 2D)                            )                                                                 \n",
            "                                                                                                  \n",
            " depthwise_conv2d_36 (Depthwise  (None, 128, 128, 32  288        ['zero_padding2d_20[0][0]']      \n",
            " Conv2D)                        )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_109 (Batch  (None, 128, 128, 32  128        ['depthwise_conv2d_36[0][0]']    \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.silu_73 (TFOpLambda)     (None, 128, 128, 32  0           ['batch_normalization_109[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_144 (Conv2D)            (None, 128, 128, 24  768         ['tf.nn.silu_73[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_110 (Batch  (None, 128, 128, 24  96         ['conv2d_144[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_145 (Conv2D)            (None, 128, 128, 48  1152        ['batch_normalization_110[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_111 (Batch  (None, 128, 128, 48  192        ['conv2d_145[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.silu_74 (TFOpLambda)     (None, 128, 128, 48  0           ['batch_normalization_111[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " depthwise_conv2d_37 (Depthwise  (None, 128, 128, 48  432        ['tf.nn.silu_74[0][0]']          \n",
            " Conv2D)                        )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_112 (Batch  (None, 128, 128, 48  192        ['depthwise_conv2d_37[0][0]']    \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.silu_75 (TFOpLambda)     (None, 128, 128, 48  0           ['batch_normalization_112[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_146 (Conv2D)            (None, 128, 128, 24  1152        ['tf.nn.silu_75[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_113 (Batch  (None, 128, 128, 24  96         ['conv2d_146[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_106 (Add)                  (None, 128, 128, 24  0           ['batch_normalization_113[0][0]',\n",
            "                                )                                 'batch_normalization_110[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_147 (Conv2D)            (None, 128, 128, 48  1152        ['add_106[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_114 (Batch  (None, 128, 128, 48  192        ['conv2d_147[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.silu_76 (TFOpLambda)     (None, 128, 128, 48  0           ['batch_normalization_114[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " depthwise_conv2d_38 (Depthwise  (None, 128, 128, 48  432        ['tf.nn.silu_76[0][0]']          \n",
            " Conv2D)                        )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_115 (Batch  (None, 128, 128, 48  192        ['depthwise_conv2d_38[0][0]']    \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.silu_77 (TFOpLambda)     (None, 128, 128, 48  0           ['batch_normalization_115[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_148 (Conv2D)            (None, 128, 128, 24  1152        ['tf.nn.silu_77[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_116 (Batch  (None, 128, 128, 24  96         ['conv2d_148[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_107 (Add)                  (None, 128, 128, 24  0           ['batch_normalization_116[0][0]',\n",
            "                                )                                 'add_106[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_149 (Conv2D)            (None, 128, 128, 48  1152        ['add_107[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_117 (Batch  (None, 128, 128, 48  192        ['conv2d_149[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.silu_78 (TFOpLambda)     (None, 128, 128, 48  0           ['batch_normalization_117[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " zero_padding2d_21 (ZeroPadding  (None, 129, 129, 48  0          ['tf.nn.silu_78[0][0]']          \n",
            " 2D)                            )                                                                 \n",
            "                                                                                                  \n",
            " depthwise_conv2d_39 (Depthwise  (None, 64, 64, 48)  432         ['zero_padding2d_21[0][0]']      \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_118 (Batch  (None, 64, 64, 48)  192         ['depthwise_conv2d_39[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " tf.nn.silu_79 (TFOpLambda)     (None, 64, 64, 48)   0           ['batch_normalization_118[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_150 (Conv2D)            (None, 64, 64, 48)   2304        ['tf.nn.silu_79[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_119 (Batch  (None, 64, 64, 48)  192         ['conv2d_150[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_151 (Conv2D)            (None, 64, 64, 64)   27712       ['batch_normalization_119[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_152 (Conv2D)            (None, 64, 64, 64)   4160        ['conv2d_151[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_30 (Reshape)           (None, 4, 1024, 64)  0           ['conv2d_152[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_90 (LayerN  (None, 4, 1024, 64)  128        ['reshape_30[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_45 (Multi  (None, 4, 1024, 64)  33216      ['layer_normalization_90[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " add_108 (Add)                  (None, 4, 1024, 64)  0           ['multi_head_attention_45[0][0]',\n",
            "                                                                  'reshape_30[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_91 (LayerN  (None, 4, 1024, 64)  128        ['add_108[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 4, 1024, 128  8320        ['layer_normalization_91[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout_90 (Dropout)           (None, 4, 1024, 128  0           ['dense_95[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dense_96 (Dense)               (None, 4, 1024, 64)  8256        ['dropout_90[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_91 (Dropout)           (None, 4, 1024, 64)  0           ['dense_96[0][0]']               \n",
            "                                                                                                  \n",
            " add_109 (Add)                  (None, 4, 1024, 64)  0           ['dropout_91[0][0]',             \n",
            "                                                                  'add_108[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_92 (LayerN  (None, 4, 1024, 64)  128        ['add_109[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_46 (Multi  (None, 4, 1024, 64)  33216      ['layer_normalization_92[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_92[0][0]'] \n",
            "                                                                                                  \n",
            " add_110 (Add)                  (None, 4, 1024, 64)  0           ['multi_head_attention_46[0][0]',\n",
            "                                                                  'add_109[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_93 (LayerN  (None, 4, 1024, 64)  128        ['add_110[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_97 (Dense)               (None, 4, 1024, 128  8320        ['layer_normalization_93[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout_92 (Dropout)           (None, 4, 1024, 128  0           ['dense_97[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dense_98 (Dense)               (None, 4, 1024, 64)  8256        ['dropout_92[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_93 (Dropout)           (None, 4, 1024, 64)  0           ['dense_98[0][0]']               \n",
            "                                                                                                  \n",
            " add_111 (Add)                  (None, 4, 1024, 64)  0           ['dropout_93[0][0]',             \n",
            "                                                                  'add_110[0][0]']                \n",
            "                                                                                                  \n",
            " reshape_31 (Reshape)           (None, 64, 64, 64)   0           ['add_111[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_153 (Conv2D)            (None, 64, 64, 48)   3120        ['reshape_31[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenate)   (None, 64, 64, 96)   0           ['batch_normalization_119[0][0]',\n",
            "                                                                  'conv2d_153[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_154 (Conv2D)            (None, 64, 64, 64)   55360       ['concatenate_15[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_155 (Conv2D)            (None, 64, 64, 128)  8192        ['conv2d_154[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_120 (Batch  (None, 64, 64, 128)  512        ['conv2d_155[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " tf.nn.silu_80 (TFOpLambda)     (None, 64, 64, 128)  0           ['batch_normalization_120[0][0]']\n",
            "                                                                                                  \n",
            " zero_padding2d_22 (ZeroPadding  (None, 65, 65, 128)  0          ['tf.nn.silu_80[0][0]']          \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " depthwise_conv2d_40 (Depthwise  (None, 32, 32, 128)  1152       ['zero_padding2d_22[0][0]']      \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_121 (Batch  (None, 32, 32, 128)  512        ['depthwise_conv2d_40[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " tf.nn.silu_81 (TFOpLambda)     (None, 32, 32, 128)  0           ['batch_normalization_121[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_156 (Conv2D)            (None, 32, 32, 64)   8192        ['tf.nn.silu_81[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_122 (Batch  (None, 32, 32, 64)  256         ['conv2d_156[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_157 (Conv2D)            (None, 32, 32, 80)   46160       ['batch_normalization_122[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_158 (Conv2D)            (None, 32, 32, 80)   6480        ['conv2d_157[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_32 (Reshape)           (None, 4, 256, 80)   0           ['conv2d_158[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_94 (LayerN  (None, 4, 256, 80)  160         ['reshape_32[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_47 (Multi  (None, 4, 256, 80)  51760       ['layer_normalization_94[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_94[0][0]'] \n",
            "                                                                                                  \n",
            " add_112 (Add)                  (None, 4, 256, 80)   0           ['multi_head_attention_47[0][0]',\n",
            "                                                                  'reshape_32[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_95 (LayerN  (None, 4, 256, 80)  160         ['add_112[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_99 (Dense)               (None, 4, 256, 160)  12960       ['layer_normalization_95[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_94 (Dropout)           (None, 4, 256, 160)  0           ['dense_99[0][0]']               \n",
            "                                                                                                  \n",
            " dense_100 (Dense)              (None, 4, 256, 80)   12880       ['dropout_94[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_95 (Dropout)           (None, 4, 256, 80)   0           ['dense_100[0][0]']              \n",
            "                                                                                                  \n",
            " add_113 (Add)                  (None, 4, 256, 80)   0           ['dropout_95[0][0]',             \n",
            "                                                                  'add_112[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_96 (LayerN  (None, 4, 256, 80)  160         ['add_113[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_48 (Multi  (None, 4, 256, 80)  51760       ['layer_normalization_96[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_96[0][0]'] \n",
            "                                                                                                  \n",
            " add_114 (Add)                  (None, 4, 256, 80)   0           ['multi_head_attention_48[0][0]',\n",
            "                                                                  'add_113[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_97 (LayerN  (None, 4, 256, 80)  160         ['add_114[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_101 (Dense)              (None, 4, 256, 160)  12960       ['layer_normalization_97[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_96 (Dropout)           (None, 4, 256, 160)  0           ['dense_101[0][0]']              \n",
            "                                                                                                  \n",
            " dense_102 (Dense)              (None, 4, 256, 80)   12880       ['dropout_96[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_97 (Dropout)           (None, 4, 256, 80)   0           ['dense_102[0][0]']              \n",
            "                                                                                                  \n",
            " add_115 (Add)                  (None, 4, 256, 80)   0           ['dropout_97[0][0]',             \n",
            "                                                                  'add_114[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_98 (LayerN  (None, 4, 256, 80)  160         ['add_115[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_49 (Multi  (None, 4, 256, 80)  51760       ['layer_normalization_98[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_98[0][0]'] \n",
            "                                                                                                  \n",
            " add_116 (Add)                  (None, 4, 256, 80)   0           ['multi_head_attention_49[0][0]',\n",
            "                                                                  'add_115[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_99 (LayerN  (None, 4, 256, 80)  160         ['add_116[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_103 (Dense)              (None, 4, 256, 160)  12960       ['layer_normalization_99[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_98 (Dropout)           (None, 4, 256, 160)  0           ['dense_103[0][0]']              \n",
            "                                                                                                  \n",
            " dense_104 (Dense)              (None, 4, 256, 80)   12880       ['dropout_98[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_99 (Dropout)           (None, 4, 256, 80)   0           ['dense_104[0][0]']              \n",
            "                                                                                                  \n",
            " add_117 (Add)                  (None, 4, 256, 80)   0           ['dropout_99[0][0]',             \n",
            "                                                                  'add_116[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_100 (Layer  (None, 4, 256, 80)  160         ['add_117[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_50 (Multi  (None, 4, 256, 80)  51760       ['layer_normalization_100[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_100[0][0]']\n",
            "                                                                                                  \n",
            " add_118 (Add)                  (None, 4, 256, 80)   0           ['multi_head_attention_50[0][0]',\n",
            "                                                                  'add_117[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_101 (Layer  (None, 4, 256, 80)  160         ['add_118[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dense_105 (Dense)              (None, 4, 256, 160)  12960       ['layer_normalization_101[0][0]']\n",
            "                                                                                                  \n",
            " dropout_100 (Dropout)          (None, 4, 256, 160)  0           ['dense_105[0][0]']              \n",
            "                                                                                                  \n",
            " dense_106 (Dense)              (None, 4, 256, 80)   12880       ['dropout_100[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_101 (Dropout)          (None, 4, 256, 80)   0           ['dense_106[0][0]']              \n",
            "                                                                                                  \n",
            " add_119 (Add)                  (None, 4, 256, 80)   0           ['dropout_101[0][0]',            \n",
            "                                                                  'add_118[0][0]']                \n",
            "                                                                                                  \n",
            " reshape_33 (Reshape)           (None, 32, 32, 80)   0           ['add_119[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_159 (Conv2D)            (None, 32, 32, 64)   5184        ['reshape_33[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 32, 32, 128)  0           ['batch_normalization_122[0][0]',\n",
            "                                                                  'conv2d_159[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_160 (Conv2D)            (None, 32, 32, 80)   92240       ['concatenate_16[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_161 (Conv2D)            (None, 32, 32, 160)  12800       ['conv2d_160[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_123 (Batch  (None, 32, 32, 160)  640        ['conv2d_161[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " tf.nn.silu_82 (TFOpLambda)     (None, 32, 32, 160)  0           ['batch_normalization_123[0][0]']\n",
            "                                                                                                  \n",
            " zero_padding2d_23 (ZeroPadding  (None, 33, 33, 160)  0          ['tf.nn.silu_82[0][0]']          \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " depthwise_conv2d_41 (Depthwise  (None, 16, 16, 160)  1440       ['zero_padding2d_23[0][0]']      \n",
            " Conv2D)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_124 (Batch  (None, 16, 16, 160)  640        ['depthwise_conv2d_41[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " tf.nn.silu_83 (TFOpLambda)     (None, 16, 16, 160)  0           ['batch_normalization_124[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_162 (Conv2D)            (None, 16, 16, 80)   12800       ['tf.nn.silu_83[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_125 (Batch  (None, 16, 16, 80)  320         ['conv2d_162[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_163 (Conv2D)            (None, 16, 16, 96)   69216       ['batch_normalization_125[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_164 (Conv2D)            (None, 16, 16, 96)   9312        ['conv2d_163[0][0]']             \n",
            "                                                                                                  \n",
            " reshape_34 (Reshape)           (None, 4, 64, 96)    0           ['conv2d_164[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_102 (Layer  (None, 4, 64, 96)   192         ['reshape_34[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_51 (Multi  (None, 4, 64, 96)   74400       ['layer_normalization_102[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_102[0][0]']\n",
            "                                                                                                  \n",
            " add_120 (Add)                  (None, 4, 64, 96)    0           ['multi_head_attention_51[0][0]',\n",
            "                                                                  'reshape_34[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_103 (Layer  (None, 4, 64, 96)   192         ['add_120[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dense_107 (Dense)              (None, 4, 64, 192)   18624       ['layer_normalization_103[0][0]']\n",
            "                                                                                                  \n",
            " dropout_102 (Dropout)          (None, 4, 64, 192)   0           ['dense_107[0][0]']              \n",
            "                                                                                                  \n",
            " dense_108 (Dense)              (None, 4, 64, 96)    18528       ['dropout_102[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_103 (Dropout)          (None, 4, 64, 96)    0           ['dense_108[0][0]']              \n",
            "                                                                                                  \n",
            " add_121 (Add)                  (None, 4, 64, 96)    0           ['dropout_103[0][0]',            \n",
            "                                                                  'add_120[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_104 (Layer  (None, 4, 64, 96)   192         ['add_121[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_52 (Multi  (None, 4, 64, 96)   74400       ['layer_normalization_104[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_104[0][0]']\n",
            "                                                                                                  \n",
            " add_122 (Add)                  (None, 4, 64, 96)    0           ['multi_head_attention_52[0][0]',\n",
            "                                                                  'add_121[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_105 (Layer  (None, 4, 64, 96)   192         ['add_122[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dense_109 (Dense)              (None, 4, 64, 192)   18624       ['layer_normalization_105[0][0]']\n",
            "                                                                                                  \n",
            " dropout_104 (Dropout)          (None, 4, 64, 192)   0           ['dense_109[0][0]']              \n",
            "                                                                                                  \n",
            " dense_110 (Dense)              (None, 4, 64, 96)    18528       ['dropout_104[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_105 (Dropout)          (None, 4, 64, 96)    0           ['dense_110[0][0]']              \n",
            "                                                                                                  \n",
            " add_123 (Add)                  (None, 4, 64, 96)    0           ['dropout_105[0][0]',            \n",
            "                                                                  'add_122[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_106 (Layer  (None, 4, 64, 96)   192         ['add_123[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_53 (Multi  (None, 4, 64, 96)   74400       ['layer_normalization_106[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_106[0][0]']\n",
            "                                                                                                  \n",
            " add_124 (Add)                  (None, 4, 64, 96)    0           ['multi_head_attention_53[0][0]',\n",
            "                                                                  'add_123[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_107 (Layer  (None, 4, 64, 96)   192         ['add_124[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dense_111 (Dense)              (None, 4, 64, 192)   18624       ['layer_normalization_107[0][0]']\n",
            "                                                                                                  \n",
            " dropout_106 (Dropout)          (None, 4, 64, 192)   0           ['dense_111[0][0]']              \n",
            "                                                                                                  \n",
            " dense_112 (Dense)              (None, 4, 64, 96)    18528       ['dropout_106[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_107 (Dropout)          (None, 4, 64, 96)    0           ['dense_112[0][0]']              \n",
            "                                                                                                  \n",
            " add_125 (Add)                  (None, 4, 64, 96)    0           ['dropout_107[0][0]',            \n",
            "                                                                  'add_124[0][0]']                \n",
            "                                                                                                  \n",
            " reshape_35 (Reshape)           (None, 16, 16, 96)   0           ['add_125[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_165 (Conv2D)            (None, 16, 16, 80)   7760        ['reshape_35[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 16, 16, 160)  0           ['batch_normalization_125[0][0]',\n",
            "                                                                  'conv2d_165[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_166 (Conv2D)            (None, 16, 16, 96)   138336      ['concatenate_17[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_167 (Conv2D)            (None, 16, 16, 320)  31040       ['conv2d_166[0][0]']             \n",
            "                                                                                                  \n",
            " global_average_pooling2d_5 (Gl  (None, 320)         0           ['conv2d_167[0][0]']             \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " dense_113 (Dense)              (None, 5)            1605        ['global_average_pooling2d_5[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,307,621\n",
            "Trainable params: 1,305,077\n",
            "Non-trainable params: 2,544\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def create_mobilevit(num_classes=5):\n",
        "    inputs = keras.Input((image_size, image_size, 3))\n",
        "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Initial conv-stem -> MV2 block.\n",
        "    x = conv_block(x, filters=16)\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=16\n",
        "    )\n",
        "\n",
        "    # Downsampling with MV2 block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "\n",
        "    # First MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=2, projection_dim=64)\n",
        "\n",
        "    # Second MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=4, projection_dim=80)\n",
        "\n",
        "    # Third MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=3, projection_dim=96)\n",
        "    x = conv_block(x, filters=320, kernel_size=1, strides=1)\n",
        "\n",
        "    # Classification head.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "mobilevit_xxs = create_mobilevit()\n",
        "mobilevit_xxs.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm46e61I2qI5"
      },
      "source": [
        "## Dataset preparation\n",
        "\n",
        "We will be using the\n",
        "[`tf_flowers`](https://www.tensorflow.org/datasets/catalog/tf_flowers)\n",
        "dataset to demonstrate the model. Unlike other Transformer-based architectures,\n",
        "MobileViT uses a simple augmentation pipeline primarily because it has the properties\n",
        "of a CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "S-Kn0Pds2qI6"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(is_training=True):\n",
        "    def _pp(image, label):\n",
        "        if is_training:\n",
        "            # Resize to a bigger spatial resolution and take the random\n",
        "            # crops.\n",
        "            image = tf.image.resize(image, (resize_bigger, resize_bigger))\n",
        "            image = tf.image.random_crop(image, (image_size, image_size, 3))\n",
        "            image = tf.image.random_flip_left_right(image)\n",
        "        else:\n",
        "            image = tf.image.resize(image, (image_size, image_size))\n",
        "        label = tf.one_hot(label, depth=num_classes)\n",
        "        return image, label\n",
        "\n",
        "    return _pp\n",
        "\n",
        "\n",
        "def prepare_dataset(x, y, is_training=True):\n",
        "    # Kreiraj TensorFlow Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "\n",
        "    if is_training:\n",
        "        # Ako je trening skup, primeni shuffling\n",
        "        dataset = dataset.shuffle(10000)\n",
        "\n",
        "    # Primena funkcije preprocess_dataset\n",
        "    dataset = dataset.map(preprocess_dataset(is_training), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Batching i prefetching\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvqhuwi-2qI6"
      },
      "source": [
        "The authors use a multi-scale data sampler to help the model learn representations of\n",
        "varied scales. In this example, we discard this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ6-OOQi2qI8"
      },
      "source": [
        "## Load and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (512, 512, 3)\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "x_val = []\n",
        "y_val = []\n",
        "\n",
        "# Učitajte slike za trening set\n",
        "for class_name in os.listdir(train_dir):\n",
        "    class_dir = os.path.join(train_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            file_path = os.path.join(class_dir, file_name)\n",
        "            #print(file_path)\n",
        "            image = cv2.imread(file_path)\n",
        "            image = cv2.resize(image, (input_shape[1], input_shape[0]))\n",
        "            x_train.append(image)\n",
        "            y_train.append([class_name])  # Convert class_name to a list\n",
        "\n",
        "# Učitajte slike za test set\n",
        "for class_name in os.listdir(test_dir):\n",
        "    class_dir = os.path.join(test_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            file_path = os.path.join(class_dir, file_name)\n",
        "            #print(file_path)\n",
        "            image = cv2.imread(file_path)\n",
        "            image = cv2.resize(image, (input_shape[1], input_shape[0]))\n",
        "            x_test.append(image)\n",
        "            y_test.append([class_name])  # Convert class_name to a list\n",
        "\n",
        "# Učitajte slike za validate set\n",
        "for class_name in os.listdir(val_dir):\n",
        "    class_dir = os.path.join(val_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            file_path = os.path.join(class_dir, file_name)\n",
        "            #print(file_path)\n",
        "            image = cv2.imread(file_path)\n",
        "            image = cv2.resize(image, (input_shape[1], input_shape[0]))\n",
        "            x_val.append(image)\n",
        "            y_val.append([class_name])  # Convert class_name to a list\n",
        "\n",
        "\n",
        "\n",
        "# Konvertujte liste u numpy nizove\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "x_val = np.array(x_val)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "# Ispis dimenzija učitanih podataka\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
        "print(f\"x_val shape: {x_val.shape} - y_val shape: {y_val.shape}\")\n",
        "\n",
        "# Convert class names to integer labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train.reshape(-1))\n",
        "y_test_encoded = label_encoder.transform(y_test.reshape(-1))\n",
        "y_val_encoded = label_encoder.transform(y_val.reshape(-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9zm8bq739We",
        "outputId": "d9c6800b-bd1a-495c-f2be-ae3badc7d574"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (923, 512, 512, 3) - y_train shape: (923, 1)\n",
            "x_test shape: (554, 512, 512, 3) - y_test shape: (554, 1)\n",
            "x_val shape: (369, 512, 512, 3) - y_val shape: (369, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pripremi skupove podataka\n",
        "train_dataset = prepare_dataset(x_train, y_train_encoded, is_training=True)\n",
        "val_dataset = prepare_dataset(x_val, y_val_encoded, is_training=False)\n",
        "test_dataset = prepare_dataset(x_test, y_test_encoded, is_training=False)\n",
        "\n",
        "# Provera\n",
        "for images, labels in train_dataset.take(1):\n",
        "  print(f\"Images batch shape: {images.shape}\")\n",
        "  print(f\"Labels batch shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkYS_-ls4Fiz",
        "outputId": "3256343a-aad8-48e3-d297-a3ca4bad634c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images batch shape: (4, 512, 512, 3)\n",
            "Labels batch shape: (4, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec1vZrkM2qI9"
      },
      "source": [
        "## Train a MobileViT (XXS) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG31Aesl2qI_",
        "outputId": "b4952a18-8a35-40c5-ad93-a77343683cea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "231/231 [==============================] - 142s 532ms/step - loss: 2.2976 - accuracy: 0.1473 - val_loss: 2.2975 - val_accuracy: 0.1030\n",
            "Epoch 2/60\n",
            "231/231 [==============================] - 117s 506ms/step - loss: 2.2400 - accuracy: 0.1387 - val_loss: 2.2895 - val_accuracy: 0.1545\n",
            "Epoch 3/60\n",
            "231/231 [==============================] - 116s 503ms/step - loss: 2.2390 - accuracy: 0.1538 - val_loss: 2.4662 - val_accuracy: 0.1491\n",
            "Epoch 4/60\n",
            "231/231 [==============================] - 117s 508ms/step - loss: 2.2232 - accuracy: 0.1387 - val_loss: 2.2228 - val_accuracy: 0.1789\n",
            "Epoch 5/60\n",
            "231/231 [==============================] - 116s 504ms/step - loss: 2.2010 - accuracy: 0.1712 - val_loss: 2.4279 - val_accuracy: 0.1653\n",
            "Epoch 6/60\n",
            "231/231 [==============================] - 116s 503ms/step - loss: 2.1915 - accuracy: 0.1885 - val_loss: 2.3652 - val_accuracy: 0.1301\n",
            "Epoch 7/60\n",
            "231/231 [==============================] - 116s 502ms/step - loss: 2.2234 - accuracy: 0.1495 - val_loss: 2.4449 - val_accuracy: 0.1924\n",
            "Epoch 8/60\n",
            "231/231 [==============================] - 115s 499ms/step - loss: 2.1744 - accuracy: 0.1820 - val_loss: 2.4370 - val_accuracy: 0.1626\n",
            "Epoch 9/60\n",
            "231/231 [==============================] - 115s 500ms/step - loss: 2.2023 - accuracy: 0.1712 - val_loss: 2.1921 - val_accuracy: 0.1599\n",
            "Epoch 10/60\n",
            "231/231 [==============================] - 115s 498ms/step - loss: 2.1531 - accuracy: 0.1950 - val_loss: 2.1438 - val_accuracy: 0.1626\n",
            "Epoch 11/60\n",
            "231/231 [==============================] - 115s 499ms/step - loss: 2.1602 - accuracy: 0.1972 - val_loss: 2.3916 - val_accuracy: 0.2060\n",
            "Epoch 12/60\n",
            "231/231 [==============================] - 115s 500ms/step - loss: 2.1345 - accuracy: 0.2037 - val_loss: 2.1009 - val_accuracy: 0.1789\n",
            "Epoch 13/60\n",
            "231/231 [==============================] - 115s 497ms/step - loss: 2.1689 - accuracy: 0.1788 - val_loss: 2.3281 - val_accuracy: 0.1545\n",
            "Epoch 14/60\n",
            "231/231 [==============================] - 116s 502ms/step - loss: 2.1633 - accuracy: 0.1950 - val_loss: 2.1823 - val_accuracy: 0.2168\n",
            "Epoch 15/60\n",
            "231/231 [==============================] - 115s 500ms/step - loss: 2.1529 - accuracy: 0.1853 - val_loss: 2.2145 - val_accuracy: 0.1951\n",
            "Epoch 16/60\n",
            "231/231 [==============================] - 115s 500ms/step - loss: 2.1503 - accuracy: 0.2059 - val_loss: 2.1823 - val_accuracy: 0.1789\n",
            "Epoch 17/60\n",
            "231/231 [==============================] - 116s 502ms/step - loss: 2.1460 - accuracy: 0.2069 - val_loss: 2.0865 - val_accuracy: 0.2276\n",
            "Epoch 18/60\n",
            "231/231 [==============================] - 115s 498ms/step - loss: 2.1476 - accuracy: 0.1972 - val_loss: 2.0932 - val_accuracy: 0.2222\n",
            "Epoch 19/60\n",
            "231/231 [==============================] - 115s 498ms/step - loss: 2.1211 - accuracy: 0.2102 - val_loss: 2.0921 - val_accuracy: 0.2033\n",
            "Epoch 20/60\n",
            "231/231 [==============================] - 115s 497ms/step - loss: 2.1113 - accuracy: 0.2210 - val_loss: 2.2222 - val_accuracy: 0.2060\n",
            "Epoch 21/60\n",
            "231/231 [==============================] - 115s 498ms/step - loss: 2.1154 - accuracy: 0.2037 - val_loss: 2.0862 - val_accuracy: 0.2195\n",
            "Epoch 22/60\n",
            "231/231 [==============================] - 115s 498ms/step - loss: 2.1155 - accuracy: 0.2405 - val_loss: 2.3029 - val_accuracy: 0.1084\n",
            "Epoch 23/60\n",
            "231/231 [==============================] - 115s 500ms/step - loss: 2.1247 - accuracy: 0.1983 - val_loss: 2.1851 - val_accuracy: 0.1978\n",
            "Epoch 24/60\n",
            "231/231 [==============================] - 116s 501ms/step - loss: 2.1055 - accuracy: 0.2254 - val_loss: 3.3459 - val_accuracy: 0.2493\n",
            "Epoch 25/60\n",
            "231/231 [==============================] - 115s 498ms/step - loss: 2.0988 - accuracy: 0.2329 - val_loss: 2.1265 - val_accuracy: 0.2114\n",
            "Epoch 26/60\n",
            "231/231 [==============================] - 115s 499ms/step - loss: 2.0865 - accuracy: 0.2243 - val_loss: 2.0369 - val_accuracy: 0.2385\n",
            "Epoch 27/60\n",
            "231/231 [==============================] - 116s 504ms/step - loss: 2.0503 - accuracy: 0.2492 - val_loss: 1.9698 - val_accuracy: 0.3089\n",
            "Epoch 28/60\n",
            "231/231 [==============================] - 116s 501ms/step - loss: 2.0632 - accuracy: 0.2611 - val_loss: 2.5955 - val_accuracy: 0.2439\n",
            "Epoch 29/60\n",
            "231/231 [==============================] - 116s 501ms/step - loss: 2.0958 - accuracy: 0.2340 - val_loss: 2.0710 - val_accuracy: 0.2222\n",
            "Epoch 30/60\n",
            "231/231 [==============================] - 115s 499ms/step - loss: 2.0563 - accuracy: 0.2438 - val_loss: 2.4658 - val_accuracy: 0.2412\n",
            "Epoch 31/60\n",
            "231/231 [==============================] - 115s 499ms/step - loss: 2.0402 - accuracy: 0.2535 - val_loss: 2.3355 - val_accuracy: 0.1057\n",
            "Epoch 32/60\n",
            "231/231 [==============================] - 115s 497ms/step - loss: 2.1603 - accuracy: 0.2059 - val_loss: 2.2970 - val_accuracy: 0.1030\n",
            "Epoch 33/60\n",
            "231/231 [==============================] - 123s 534ms/step - loss: 2.1296 - accuracy: 0.2015 - val_loss: 2.0960 - val_accuracy: 0.2033\n",
            "Epoch 34/60\n",
            "231/231 [==============================] - 115s 498ms/step - loss: 2.0701 - accuracy: 0.2210 - val_loss: 2.0854 - val_accuracy: 0.2141\n",
            "Epoch 35/60\n",
            "231/231 [==============================] - 115s 496ms/step - loss: 2.0674 - accuracy: 0.2373 - val_loss: 2.0500 - val_accuracy: 0.2710\n",
            "Epoch 36/60\n",
            "231/231 [==============================] - 115s 496ms/step - loss: 2.0492 - accuracy: 0.2687 - val_loss: 2.1173 - val_accuracy: 0.2168\n",
            "Epoch 37/60\n",
            "231/231 [==============================] - 115s 499ms/step - loss: 2.0436 - accuracy: 0.2633 - val_loss: 2.0701 - val_accuracy: 0.2114\n",
            "Epoch 38/60\n",
            "231/231 [==============================] - 115s 498ms/step - loss: 2.0211 - accuracy: 0.2719 - val_loss: 1.9596 - val_accuracy: 0.2981\n",
            "Epoch 39/60\n",
            "231/231 [==============================] - 114s 495ms/step - loss: 2.1842 - accuracy: 0.1993 - val_loss: 2.1929 - val_accuracy: 0.1491\n",
            "Epoch 40/60\n",
            "231/231 [==============================] - 114s 495ms/step - loss: 2.0995 - accuracy: 0.2449 - val_loss: 2.1787 - val_accuracy: 0.1680\n",
            "Epoch 41/60\n",
            "231/231 [==============================] - 114s 495ms/step - loss: 2.0408 - accuracy: 0.2611 - val_loss: 1.9808 - val_accuracy: 0.2710\n",
            "Epoch 42/60\n",
            "231/231 [==============================] - 114s 493ms/step - loss: 1.9834 - accuracy: 0.2893 - val_loss: 1.9104 - val_accuracy: 0.2927\n",
            "Epoch 43/60\n",
            "231/231 [==============================] - 114s 494ms/step - loss: 1.9029 - accuracy: 0.3185 - val_loss: 1.9767 - val_accuracy: 0.2846\n",
            "Epoch 44/60\n",
            "231/231 [==============================] - 114s 495ms/step - loss: 1.8941 - accuracy: 0.3283 - val_loss: 2.4317 - val_accuracy: 0.3279\n",
            "Epoch 45/60\n",
            "231/231 [==============================] - 115s 496ms/step - loss: 1.9157 - accuracy: 0.3348 - val_loss: 1.8372 - val_accuracy: 0.3198\n",
            "Epoch 46/60\n",
            "231/231 [==============================] - 114s 494ms/step - loss: 1.8740 - accuracy: 0.3424 - val_loss: 6.0048 - val_accuracy: 0.2358\n",
            "Epoch 47/60\n",
            "231/231 [==============================] - 115s 498ms/step - loss: 1.9312 - accuracy: 0.3120 - val_loss: 1.8361 - val_accuracy: 0.3848\n",
            "Epoch 48/60\n",
            "231/231 [==============================] - 115s 498ms/step - loss: 1.8633 - accuracy: 0.3619 - val_loss: 1.7352 - val_accuracy: 0.4390\n",
            "Epoch 49/60\n",
            "202/231 [=========================>....] - ETA: 12s - loss: 1.8545 - accuracy: 0.3738"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.002\n",
        "label_smoothing_factor = 0.1\n",
        "epochs = 60 #30\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing_factor)\n",
        "\n",
        "\n",
        "def run_experiment(epochs=epochs):\n",
        "    mobilevit_xxs = create_mobilevit(num_classes=num_classes)\n",
        "    mobilevit_xxs.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    mobilevit_xxs.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=epochs,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "    mobilevit_xxs.load_weights(checkpoint_filepath)\n",
        "    _, accuracy = mobilevit_xxs.evaluate(val_dataset)\n",
        "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    #########################################################\n",
        "    _, test_accuracy = mobilevit_xxs.evaluate(test_dataset)\n",
        "    print(f\"Test accuracy: {round(test_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return mobilevit_xxs\n",
        "\n",
        "\n",
        "mobilevit_xxs = run_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HgjsoBh2qI_"
      },
      "source": [
        "## Results and TFLite conversion\n",
        "\n",
        "With about one million parameters, getting to ~85% top-1 accuracy on 256x256 512x512* resolution is\n",
        "a strong result. This MobileViT mobile is fully compatible with TensorFlow Lite (TFLite)\n",
        "and can be converted with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKbqXcxw2qJA"
      },
      "outputs": [],
      "source": [
        "# Serialize the model as a SavedModel.\n",
        "mobilevit_xxs.save(\"mobilevit_xxs\")\n",
        "\n",
        "# Convert to TFLite. This form of quantization is called\n",
        "# post-training dynamic-range quantization in TFLite.\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"mobilevit_xxs\")\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS,  # Enable TensorFlow ops.\n",
        "]\n",
        "tflite_model = converter.convert()\n",
        "open(\"mobilevit_xxs.tflite\", \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QOefpFX2qJA"
      },
      "source": [
        "To learn more about different quantization recipes available in TFLite and running\n",
        "inference with TFLite models, check out\n",
        "[this official resource](https://www.tensorflow.org/lite/performance/post_training_quantization).\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/mobile-vit-xxs) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/Flowers-Classification-MobileViT)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}