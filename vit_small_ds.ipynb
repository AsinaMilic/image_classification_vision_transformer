{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO8mvr6HaSgI"
      },
      "source": [
        "# Train a Vision Transformer on small datasets\n",
        "\n",
        "# NE ZNAM STO MI DAJE OVAKVE REZULTATE..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba1GtcuFaSgL"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In the academic paper\n",
        "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929),\n",
        "the authors mention that Vision Transformers (ViT) are data-hungry. Therefore,\n",
        "pretraining a ViT on a large-sized dataset like JFT300M and fine-tuning\n",
        "it on medium-sized datasets (like ImageNet) is the only way to beat\n",
        "state-of-the-art Convolutional Neural Network models.\n",
        "\n",
        "The self-attention layer of ViT lacks **locality inductive bias** (the notion that\n",
        "image pixels are locally correlated and that their correlation maps are translation-invariant).\n",
        "This is the reason why ViTs need more data. On the other hand, CNNs look at images through\n",
        "spatial sliding windows, which helps them get better results with smaller datasets.\n",
        "\n",
        "In the academic paper\n",
        "[Vision Transformer for Small-Size Datasets](https://arxiv.org/abs/2112.13492v1),\n",
        "the authors set out to tackle the problem of locality inductive bias in ViTs.\n",
        "\n",
        "The main ideas are:\n",
        "\n",
        "- **Shifted Patch Tokenization**\n",
        "- **Locality Self Attention**\n",
        "\n",
        "This example implements the ideas of the paper. A large part of this\n",
        "example is inspired from\n",
        "[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).\n",
        "\n",
        "_Note_: This example requires TensorFlow 2.6 or higher, as well as\n",
        "[TensorFlow Addons](https://www.tensorflow.org/addons), which can be\n",
        "installed using the following command:\n",
        "\n",
        "```python\n",
        "pip install -qq -U tensorflow-addons\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqTKa5mTaSgN"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install tensorflow==2.8.0\n",
        "!pip install opencv-python\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itmUZXY9qPIO",
        "outputId": "fd0169a7-e3d5-497f-8e41-7c09ea6f62ca"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: tensorflow==2.8.0 in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (23.5.26)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.32.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.56.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.41.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7r6cDfhaSgP",
        "outputId": "bebd54be-fc6f-4a8c-cde9-9bd78f8130ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "!pip install tensorflow-addons\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "# Setting seed for reproducibiltiy\n",
        "SEED = 42\n",
        "keras.utils.set_random_seed(SEED)\n",
        "\n",
        "# Setting mixed precision policy\n",
        "#tf.keras.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MheSmQkTaSgR"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiGeRRvUaSgS",
        "outputId": "a26a7784-2fb5-4ef8-c122-bf2e2165b26d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (130, 512, 512, 3) - y_train shape: (130, 1)\n",
            "x_test shape: (130, 512, 512, 3) - y_test shape: (130, 1)\n",
            "x_val shape: (130, 512, 512, 3) - y_val shape: (130, 1)\n"
          ]
        }
      ],
      "source": [
        "#NUM_CLASSES = 100\n",
        "#INPUT_SHAPE = (32, 32, 3)\n",
        "\n",
        "#(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "input_shape = (512, 512, 3)\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/Chiro10_Smaller/train_512'\n",
        "test_dir = '/content/drive/MyDrive/Chiro10_Smaller/test_512'\n",
        "val_dir = '/content/drive/MyDrive/Chiro10_Smaller/val_512'\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "x_val = []\n",
        "y_val = []\n",
        "\n",
        "# Učitajte slike za trening set\n",
        "for class_name in os.listdir(train_dir):\n",
        "    class_dir = os.path.join(train_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            file_path = os.path.join(class_dir, file_name)\n",
        "            #print(file_path)\n",
        "            image = cv2.imread(file_path)\n",
        "            image = cv2.resize(image, (input_shape[1], input_shape[0]))\n",
        "            x_train.append(image)\n",
        "            y_train.append([class_name])  # Convert class_name to a list\n",
        "\n",
        "# Učitajte slike za test set\n",
        "for class_name in os.listdir(test_dir):\n",
        "    class_dir = os.path.join(test_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            file_path = os.path.join(class_dir, file_name)\n",
        "            #print(file_path)\n",
        "            image = cv2.imread(file_path)\n",
        "            image = cv2.resize(image, (input_shape[1], input_shape[0]))\n",
        "            x_test.append(image)\n",
        "            y_test.append([class_name])  # Convert class_name to a list\n",
        "\n",
        "# Učitajte slike za validate set\n",
        "for class_name in os.listdir(val_dir):\n",
        "    class_dir = os.path.join(val_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            file_path = os.path.join(class_dir, file_name)\n",
        "            #print(file_path)\n",
        "            image = cv2.imread(file_path)\n",
        "            image = cv2.resize(image, (input_shape[1], input_shape[0]))\n",
        "            x_val.append(image)\n",
        "            y_val.append([class_name])  # Convert class_name to a list\n",
        "\n",
        "\n",
        "\n",
        "# Konvertujte liste u numpy nizove\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "x_val = np.array(x_val)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "# Ispis dimenzija učitanih podataka\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
        "print(f\"x_val shape: {x_val.shape} - y_val shape: {y_val.shape}\")\n",
        "\n",
        "# Convert class names to integer labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train.reshape(-1))\n",
        "y_test_encoded = label_encoder.transform(y_test.reshape(-1))\n",
        "y_val_encoded = label_encoder.transform(y_val.reshape(-1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDTiPh0baSgU"
      },
      "source": [
        "## Configure the hyperparameters\n",
        "\n",
        "The hyperparameters are different from the paper. Feel free to tune\n",
        "the hyperparameters yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "LziVKoJLaSgV"
      },
      "outputs": [],
      "source": [
        "# DATA\n",
        "BUFFER_SIZE = 512 #512\n",
        "BATCH_SIZE =  4#nece za 8 16 32. #original - 256\n",
        "\n",
        "# AUGMENTATION\n",
        "IMAGE_SIZE = 580 #72\n",
        "PATCH_SIZE = 32#6\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 0.001 #0.001\n",
        "WEIGHT_DECAY = 0.0001\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 10\n",
        "\n",
        "# ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "TRANSFORMER_LAYERS = 8\n",
        "PROJECTION_DIM = 64\n",
        "NUM_HEADS = 4\n",
        "TRANSFORMER_UNITS = [\n",
        "    PROJECTION_DIM * 2,\n",
        "    PROJECTION_DIM,\n",
        "]\n",
        "MLP_HEAD_UNITS = [2048, 1024]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITrirc4jaSgX"
      },
      "source": [
        "## Use data augmentation\n",
        "\n",
        "A snippet from the paper:\n",
        "\n",
        "*\"According to DeiT, various techniques are required to effectively\n",
        "train ViTs. Thus, we applied data augmentations such as CutMix, Mixup,\n",
        "Auto Augment, Repeated Augment to all models.\"*\n",
        "\n",
        "In this example, we will focus solely on the novelty of the approach\n",
        "and not on reproducing the paper results. For this reason, we\n",
        "don't use the mentioned data augmentation schemes. Please feel\n",
        "free to add to or remove from the augmentation pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "1NkZ_QBMaSgZ"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFaz7HpNaSga"
      },
      "source": [
        "## Implement Shifted Patch Tokenization\n",
        "\n",
        "In a ViT pipeline, the input images are divided into patches that are\n",
        "then linearly projected into tokens. Shifted patch tokenization (STP)\n",
        "is introduced to combat the low receptive field of ViTs. The steps\n",
        "for Shifted Patch Tokenization are as follows:\n",
        "\n",
        "- Start with an image.\n",
        "- Shift the image in diagonal directions.\n",
        "- Concat the diagonally shifted images with the original image.\n",
        "- Extract patches of the concatenated images.\n",
        "- Flatten the spatial dimension of all patches.\n",
        "- Layer normalize the flattened patches and then project it.\n",
        "\n",
        "| ![Shifted Patch Toekenization](https://i.imgur.com/bUnHxd0.png) |\n",
        "| :--: |\n",
        "| Shifted Patch Tokenization [Source](https://arxiv.org/abs/2112.13492v1) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "CbowyTtVaSgb"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ShiftedPatchTokenization(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_patches=NUM_PATCHES,\n",
        "        projection_dim=PROJECTION_DIM,\n",
        "        vanilla=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.half_patch = patch_size // 2\n",
        "        self.flatten_patches = layers.Reshape((num_patches, -1))\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
        "\n",
        "    def crop_shift_pad(self, images, mode):\n",
        "        # Build the diagonally shifted images\n",
        "        if mode == \"left-up\":\n",
        "            crop_height = self.half_patch\n",
        "            crop_width = self.half_patch\n",
        "            shift_height = 0\n",
        "            shift_width = 0\n",
        "        elif mode == \"left-down\":\n",
        "            crop_height = 0\n",
        "            crop_width = self.half_patch\n",
        "            shift_height = self.half_patch\n",
        "            shift_width = 0\n",
        "        elif mode == \"right-up\":\n",
        "            crop_height = self.half_patch\n",
        "            crop_width = 0\n",
        "            shift_height = 0\n",
        "            shift_width = self.half_patch\n",
        "        else:\n",
        "            crop_height = 0\n",
        "            crop_width = 0\n",
        "            shift_height = self.half_patch\n",
        "            shift_width = self.half_patch\n",
        "\n",
        "        # Crop the shifted images and pad them\n",
        "        crop = tf.image.crop_to_bounding_box(\n",
        "            images,\n",
        "            offset_height=crop_height,\n",
        "            offset_width=crop_width,\n",
        "            target_height=self.image_size - self.half_patch,\n",
        "            target_width=self.image_size - self.half_patch,\n",
        "        )\n",
        "        shift_pad = tf.image.pad_to_bounding_box(\n",
        "            crop,\n",
        "            offset_height=shift_height,\n",
        "            offset_width=shift_width,\n",
        "            target_height=self.image_size,\n",
        "            target_width=self.image_size,\n",
        "        )\n",
        "        return shift_pad\n",
        "\n",
        "    def call(self, images):\n",
        "        if not self.vanilla:\n",
        "            # Concat the shifted images with the original image\n",
        "            images = tf.concat(\n",
        "                [\n",
        "                    images,\n",
        "                    self.crop_shift_pad(images, mode=\"left-up\"),\n",
        "                    self.crop_shift_pad(images, mode=\"left-down\"),\n",
        "                    self.crop_shift_pad(images, mode=\"right-up\"),\n",
        "                    self.crop_shift_pad(images, mode=\"right-down\"),\n",
        "                ],\n",
        "                axis=-1,\n",
        "            )\n",
        "        # Patchify the images and flatten it\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        flat_patches = self.flatten_patches(patches)\n",
        "        if not self.vanilla:\n",
        "            # Layer normalize the flat patches and linearly project it\n",
        "            tokens = self.layer_norm(flat_patches)\n",
        "            tokens = self.projection(tokens)\n",
        "        else:\n",
        "            # Linearly project the flat patches\n",
        "            tokens = self.projection(flat_patches)\n",
        "        return (tokens, patches)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JFEWnDMaSge"
      },
      "source": [
        "## Implement the patch encoding layer\n",
        "\n",
        "This layer accepts projected patches and then adds positional\n",
        "information to them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "Fcpgmed2aSge"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(\n",
        "        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patches = num_patches\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "\n",
        "    def call(self, encoded_patches):\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_patches = encoded_patches + encoded_positions\n",
        "        return encoded_patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKGaLLruaSgf"
      },
      "source": [
        "## Implement Locality Self Attention\n",
        "\n",
        "The regular attention equation is stated below.\n",
        "\n",
        "| ![Equation of attention](https://miro.medium.com/max/396/1*P9sV1xXM10t943bXy_G9yg.png) |\n",
        "| :--: |\n",
        "| [Source](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634) |\n",
        "\n",
        "The attention module takes a query, key, and value. First, we compute the\n",
        "similarity between the query and key via a dot product. Then, the result\n",
        "is scaled by the square root of the key dimension. The scaling prevents\n",
        "the softmax function from having an overly small gradient. Softmax is then\n",
        "applied to the scaled dot product to produce the attention weights.\n",
        "The value is then modulated via the attention weights.\n",
        "\n",
        "In self-attention, query, key and value come from the same input.\n",
        "The dot product would result in large self-token relations rather than\n",
        "inter-token relations. This also means that the softmax gives higher\n",
        "probabilities to self-token relations than the inter-token relations.\n",
        "To combat this, the authors propose masking the diagonal of the dot product.\n",
        "This way, we force the attention module to pay more attention to the\n",
        "inter-token relations.\n",
        "\n",
        "The scaling factor is a constant in the regular attention module.\n",
        "This acts like a temperature term that can modulate the softmax function.\n",
        "The authors suggest a learnable temperature term instead of a constant.\n",
        "\n",
        "| ![Implementation of LSA](https://i.imgur.com/GTV99pk.png) |\n",
        "| :--: |\n",
        "| Locality Self Attention [Source](https://arxiv.org/abs/2112.13492v1) |\n",
        "\n",
        "The above two pointers make the Locality Self Attention. We have subclassed the\n",
        "[`layers.MultiHeadAttention`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)\n",
        "and implemented the trainable temperature. The attention mask is built\n",
        "at a later stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "E7GEALiXaSgg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # The trainable temperature term. The initial value is\n",
        "        # the square root of the key dimension.                 #dodato\n",
        "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), dtype=tf.float16, trainable=True)\n",
        "\n",
        "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
        "        query = tf.multiply(query, 1.0 / self.tau)\n",
        "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
        "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
        "        attention_scores_dropout = self._dropout_layer(\n",
        "            attention_scores, training=training\n",
        "        )\n",
        "        attention_output = tf.einsum(\n",
        "            self._combine_equation, attention_scores_dropout, value\n",
        "        )\n",
        "        return attention_output, attention_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfHz0Xl7aSgg"
      },
      "source": [
        "## Implement the MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "A3550txkaSgh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "# Build the diagonal attention mask\n",
        "diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
        "diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqg3tBubaSgh"
      },
      "source": [
        "## Build the ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "cCIIMaMEaSgh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_vit_classifier(vanilla=False):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder()(tokens)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(TRANSFORMER_LAYERS):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        if not vanilla:\n",
        "            attention_output = MultiHeadAttentionLSA(\n",
        "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
        "            )(x1, x1, attention_mask=diag_attn_mask)\n",
        "        else:\n",
        "            attention_output = layers.MultiHeadAttention(\n",
        "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
        "            )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGRUKP3zaSgi"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mCu_xDBaSgj",
        "outputId": "cc76cca0-1d9c-47b6-a23b-12da7b4a1085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "33/33 [==============================] - 32s 118ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "33/33 [==============================] - 2s 71ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "33/33 [==============================] - 4s 109ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "33/33 [==============================] - 3s 90ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "33/33 [==============================] - 2s 65ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "33/33 [==============================] - 2s 68ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "33/33 [==============================] - 2s 65ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "33/33 [==============================] - 2s 65ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "33/33 [==============================] - 3s 83ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "33/33 [==============================] - 3s 93ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "33/33 [==============================] - 1s 31ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00\n",
            "Test accuracy: 10.0%\n",
            "Test top 5 accuracy: 0.0%\n",
            "Epoch 1/10\n",
            "33/33 [==============================] - 24s 152ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "33/33 [==============================] - 3s 105ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "33/33 [==============================] - 4s 121ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "33/33 [==============================] - 3s 102ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "33/33 [==============================] - 3s 104ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "33/33 [==============================] - 3s 96ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "33/33 [==============================] - 4s 112ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "33/33 [==============================] - 4s 125ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "33/33 [==============================] - 3s 96ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "33/33 [==============================] - 3s 106ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.1000 - val_top-5-accuracy: 0.0000e+00\n",
            "33/33 [==============================] - 1s 41ms/step - loss: nan - accuracy: 0.1000 - top-5-accuracy: 0.0000e+00\n",
            "Test accuracy: 10.0%\n",
            "Test top 5 accuracy: 0.0%\n"
          ]
        }
      ],
      "source": [
        "# Some code is taken from:\n",
        "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n",
        "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.learning_rate_base = learning_rate_base\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_learning_rate = warmup_learning_rate\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.pi = tf.constant(np.pi)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        if self.total_steps < self.warmup_steps:\n",
        "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
        "\n",
        "        cos_annealed_lr = tf.cos(\n",
        "            self.pi\n",
        "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
        "            / float(self.total_steps - self.warmup_steps)\n",
        "        )\n",
        "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
        "\n",
        "        if self.warmup_steps > 0:\n",
        "            if self.learning_rate_base < self.warmup_learning_rate:\n",
        "                raise ValueError(\n",
        "                    \"Learning_rate_base must be larger or equal to \"\n",
        "                    \"warmup_learning_rate.\"\n",
        "                )\n",
        "            slope = (\n",
        "                self.learning_rate_base - self.warmup_learning_rate\n",
        "            ) / self.warmup_steps\n",
        "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
        "            learning_rate = tf.where(\n",
        "                step < self.warmup_steps, warmup_rate, learning_rate\n",
        "            )\n",
        "        return tf.where(\n",
        "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
        "        )\n",
        "\n",
        "\n",
        "def run_experiment(model):\n",
        "    total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)\n",
        "    warmup_epoch_percentage = 0.10\n",
        "    warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
        "    scheduled_lrs = WarmUpCosine(\n",
        "        learning_rate_base=LEARNING_RATE,\n",
        "        total_steps=total_steps,\n",
        "        warmup_learning_rate=0.0,\n",
        "        warmup_steps=warmup_steps,\n",
        "    )\n",
        "\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train_encoded,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=(x_val, y_val_encoded),\n",
        "    )\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test_encoded, batch_size=BATCH_SIZE)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "# Run experiments with the vanilla ViT\n",
        "vit = create_vit_classifier(vanilla=True)\n",
        "history = run_experiment(vit)\n",
        "\n",
        "# Run experiments with the Shifted Patch Tokenization and\n",
        "# Locality Self Attention modified ViT\n",
        "vit_sl = create_vit_classifier(vanilla=False)\n",
        "history = run_experiment(vit_sl)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vit_small_ds",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}