{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLQMjCgkNd-M"
      },
      "source": [
        "# Image classification with Swin Transformers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqX_TBsENd-P"
      },
      "source": [
        "This example implements [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n",
        "by Liu et al. for image classification, and demonstrates it on the\n",
        "[CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "Swin Transformer (**S**hifted **Win**dow Transformer) can serve as a general-purpose backbone\n",
        "for computer vision. Swin Transformer is a hierarchical Transformer whose\n",
        "representations are computed with _shifted windows_. The shifted window scheme\n",
        "brings greater efficiency by limiting self-attention computation to\n",
        "non-overlapping local windows while also allowing for cross-window connections.\n",
        "This architecture has the flexibility to model information at various scales and has\n",
        "a linear computational complexity with respect to image size.\n",
        "\n",
        "This example requires TensorFlow 2.5 or higher, as well as TensorFlow Addons,\n",
        "which can be installed using the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEt1QHUiNd-R",
        "outputId": "93cda03c-9760-47f8-d41e-ff4f2916e4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n",
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (23.5.26)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.8.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (4.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.14.1)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.0)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow==2.8.0)\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.0)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (0.32.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.0) (1.56.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.0)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.1\n",
            "    Uninstalling tensorboard-data-server-0.7.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip install tensorflow==2.8.0\n",
        "!pip install opencv-python\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQtuymrPNd-U"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kF0sYGVONd-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab8a74e-7174-477b-b03c-f64837aaae1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0pLZdhzNd-X"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "We load the CIFAR-100 dataset through `tf.keras.datasets`,\n",
        "normalize the images, and convert the integer labels to one-hot encoded vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h8ByI28fNd-Y"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "input_shape = (512, 512, 3)\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/Chiro10/train_512'\n",
        "test_dir = '/content/drive/MyDrive/Chiro10/test_512'\n",
        "val_dir = '/content/drive/MyDrive/Chiro10/val_512'\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "x_val = []\n",
        "y_val = []\n",
        "\n",
        "# Load images for the training set\n",
        "for class_name in os.listdir(train_dir):\n",
        "    class_dir = os.path.join(train_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            file_path = os.path.join(class_dir, file_name)\n",
        "            image = cv2.imread(file_path)\n",
        "            image = cv2.resize(image, (input_shape[1], input_shape[0]))\n",
        "            x_train.append(image)\n",
        "            y_train.append(class_name)  # Store class_name as is\n",
        "\n",
        "# Load images for the test set\n",
        "for class_name in os.listdir(test_dir):\n",
        "    class_dir = os.path.join(test_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            file_path = os.path.join(class_dir, file_name)\n",
        "            image = cv2.imread(file_path)\n",
        "            image = cv2.resize(image, (input_shape[1], input_shape[0]))\n",
        "            x_test.append(image)\n",
        "            y_test.append(class_name)  # Store class_name as is\n",
        "\n",
        "# Load images for the validation set\n",
        "for class_name in os.listdir(val_dir):\n",
        "    class_dir = os.path.join(val_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            file_path = os.path.join(class_dir, file_name)\n",
        "            image = cv2.imread(file_path)\n",
        "            image = cv2.resize(image, (input_shape[1], input_shape[0]))\n",
        "            x_val.append(image)\n",
        "            y_val.append(class_name)  # Store class_name as is\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)"
      ],
      "metadata": {
        "id": "6lPNHxYqn8y1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "GqlsD9I3n8hU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = np.array(x_val)\n",
        "y_val = np.array(y_val)"
      ],
      "metadata": {
        "id": "zXM2ZB9loxnq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the pixel values to [0, 1]\n",
        "#x_train = x_train / 255.0\n",
        "#x_test = x_test / 255.0\n",
        "#x_val = x_val / 255.0 #PREVAZILAZI 12GB rama ovde..."
      ],
      "metadata": {
        "id": "gx6h24x70sb_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convert class names to integer labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "y_val_encoded = label_encoder.transform(y_val)\n",
        "\n",
        "# One-hot encode the target labels\n",
        "y_train = keras.utils.to_categorical(y_train_encoded, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test_encoded, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val_encoded, num_classes)\n",
        "\n",
        "# Print the shapes of loaded data\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
        "print(f\"x_val shape: {x_val.shape} - y_val shape: {y_val.shape}\")"
      ],
      "metadata": {
        "id": "n8ULimZCngRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46c9308-a127-49b2-9b97-dae5ffb49470"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (923, 512, 512, 3) - y_train shape: (923, 10)\n",
            "x_test shape: (554, 512, 512, 3) - y_test shape: (554, 10)\n",
            "x_val shape: (369, 512, 512, 3) - y_val shape: (369, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNtagAogNd-Z"
      },
      "source": [
        "## Configure the hyperparameters\n",
        "\n",
        "A key parameter to pick is the `patch_size`, the size of the input patches.\n",
        "In order to use each pixel as an individual input, you can set `patch_size` to `(1, 1)`.\n",
        "Below, we take inspiration from the original paper settings\n",
        "for training on ImageNet-1K, keeping most of the original settings for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Y-L82MguNd-a"
      },
      "outputs": [],
      "source": [
        "patch_size = (2, 2)  # 2-by-2 sized patches\n",
        "dropout_rate = 0.03  # Dropout rate\n",
        "num_heads = 8  # Attention heads\n",
        "embed_dim = 64  # Embedding dimension\n",
        "num_mlp = 256  # MLP layer size\n",
        "qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\n",
        "window_size = 2  # Size of attention window\n",
        "shift_size = 1  # Size of shifting window\n",
        "image_dimension = 512  # Initial image size\n",
        "\n",
        "num_patch_x = input_shape[0] // patch_size[0]\n",
        "num_patch_y = input_shape[1] // patch_size[1]\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 128\n",
        "num_epochs = 40 #40\n",
        "validation_split = 0.1\n",
        "weight_decay = 0.0001\n",
        "label_smoothing = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc35H6tqNd-c"
      },
      "source": [
        "## Helper functions\n",
        "\n",
        "We create two helper functions to help us get a sequence of\n",
        "patches from the image, merge patches, and apply dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bO7ftqwnNd-d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def window_partition(x, window_size):\n",
        "    _, height, width, channels = x.shape\n",
        "    patch_num_y = height // window_size\n",
        "    patch_num_x = width // window_size\n",
        "    x = tf.reshape(\n",
        "        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n",
        "    )\n",
        "    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n",
        "    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n",
        "    return windows\n",
        "\n",
        "\n",
        "def window_reverse(windows, window_size, height, width, channels):\n",
        "    patch_num_y = height // window_size\n",
        "    patch_num_x = width // window_size\n",
        "    x = tf.reshape(\n",
        "        windows,\n",
        "        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n",
        "    )\n",
        "    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n",
        "    x = tf.reshape(x, shape=(-1, height, width, channels))\n",
        "    return x\n",
        "\n",
        "\n",
        "class DropPath(layers.Layer):\n",
        "    def __init__(self, drop_prob=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def call(self, x):\n",
        "        input_shape = tf.shape(x)\n",
        "        batch_size = input_shape[0]\n",
        "        rank = x.shape.rank\n",
        "        shape = (batch_size,) + (1,) * (rank - 1)\n",
        "        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n",
        "        path_mask = tf.floor(random_tensor)\n",
        "        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr6SYirMNd-f"
      },
      "source": [
        "## Window based multi-head self-attention\n",
        "\n",
        "Usually Transformers perform global self-attention, where the relationships between\n",
        "a token and all other tokens are computed. The global computation leads to quadratic\n",
        "complexity with respect to the number of tokens. Here, as the [original paper](https://arxiv.org/abs/2103.14030)\n",
        "suggests, we compute self-attention within local windows, in a non-overlapping manner.\n",
        "Global self-attention leads to quadratic computational complexity in the number of patches,\n",
        "whereas window-based self-attention leads to linear complexity and is easily scalable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1lw6PL9rNd-f"
      },
      "outputs": [],
      "source": [
        "\n",
        "class WindowAttention(layers.Layer):\n",
        "    def __init__(\n",
        "        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.proj = layers.Dense(dim)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        num_window_elements = (2 * self.window_size[0] - 1) * (\n",
        "            2 * self.window_size[1] - 1\n",
        "        )\n",
        "        self.relative_position_bias_table = self.add_weight(\n",
        "            shape=(num_window_elements, self.num_heads),\n",
        "            initializer=tf.initializers.Zeros(),\n",
        "            trainable=True,\n",
        "        )\n",
        "        coords_h = np.arange(self.window_size[0])\n",
        "        coords_w = np.arange(self.window_size[1])\n",
        "        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n",
        "        coords = np.stack(coords_matrix)\n",
        "        coords_flatten = coords.reshape(2, -1)\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)\n",
        "\n",
        "        self.relative_position_index = tf.Variable(\n",
        "            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False\n",
        "        )\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        _, size, channels = x.shape\n",
        "        head_dim = channels // self.num_heads\n",
        "        x_qkv = self.qkv(x)\n",
        "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n",
        "        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n",
        "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
        "        q = q * self.scale\n",
        "        k = tf.transpose(k, perm=(0, 1, 3, 2))\n",
        "        attn = q @ k\n",
        "\n",
        "        num_window_elements = self.window_size[0] * self.window_size[1]\n",
        "        relative_position_index_flat = tf.reshape(\n",
        "            self.relative_position_index, shape=(-1,)\n",
        "        )\n",
        "        relative_position_bias = tf.gather(\n",
        "            self.relative_position_bias_table, relative_position_index_flat\n",
        "        )\n",
        "        relative_position_bias = tf.reshape(\n",
        "            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n",
        "        )\n",
        "        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n",
        "        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.get_shape()[0]\n",
        "            mask_float = tf.cast(\n",
        "                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n",
        "            )\n",
        "            attn = (\n",
        "                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n",
        "                + mask_float\n",
        "            )\n",
        "            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n",
        "            attn = keras.activations.softmax(attn, axis=-1)\n",
        "        else:\n",
        "            attn = keras.activations.softmax(attn, axis=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        x_qkv = attn @ v\n",
        "        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n",
        "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n",
        "        x_qkv = self.proj(x_qkv)\n",
        "        x_qkv = self.dropout(x_qkv)\n",
        "        return x_qkv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THfzZ90CNd-g"
      },
      "source": [
        "## The complete Swin Transformer model\n",
        "\n",
        "Finally, we put together the complete Swin Transformer by replacing the standard multi-head\n",
        "attention (MHA) with shifted windows attention. As suggested in the\n",
        "original paper, we create a model comprising of a shifted window-based MHA\n",
        "layer, followed by a 2-layer MLP with GELU nonlinearity in between, applying\n",
        "`LayerNormalization` before each MSA layer and each MLP, and a residual\n",
        "connection after each of these layers.\n",
        "\n",
        "Notice that we only create a simple MLP with 2 Dense and\n",
        "2 Dropout layers. Often you will see models using ResNet-50 as the MLP which is\n",
        "quite standard in the literature. However in this paper the authors use a\n",
        "2-layer MLP with GELU nonlinearity in between."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8039vck0Nd-h"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SwinTransformer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_patch,\n",
        "        num_heads,\n",
        "        window_size=7,\n",
        "        shift_size=0,\n",
        "        num_mlp=1024,\n",
        "        qkv_bias=True,\n",
        "        dropout_rate=0.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.dim = dim  # number of input dimensions\n",
        "        self.num_patch = num_patch  # number of embedded patches\n",
        "        self.num_heads = num_heads  # number of attention heads\n",
        "        self.window_size = window_size  # size of window\n",
        "        self.shift_size = shift_size  # size of window shift\n",
        "        self.num_mlp = num_mlp  # number of MLP nodes\n",
        "\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.attn = WindowAttention(\n",
        "            dim,\n",
        "            window_size=(self.window_size, self.window_size),\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        self.drop_path = DropPath(dropout_rate)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
        "\n",
        "        self.mlp = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(num_mlp),\n",
        "                layers.Activation(keras.activations.gelu),\n",
        "                layers.Dropout(dropout_rate),\n",
        "                layers.Dense(dim),\n",
        "                layers.Dropout(dropout_rate),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if min(self.num_patch) < self.window_size:\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.num_patch)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.shift_size == 0:\n",
        "            self.attn_mask = None\n",
        "        else:\n",
        "            height, width = self.num_patch\n",
        "            h_slices = (\n",
        "                slice(0, -self.window_size),\n",
        "                slice(-self.window_size, -self.shift_size),\n",
        "                slice(-self.shift_size, None),\n",
        "            )\n",
        "            w_slices = (\n",
        "                slice(0, -self.window_size),\n",
        "                slice(-self.window_size, -self.shift_size),\n",
        "                slice(-self.shift_size, None),\n",
        "            )\n",
        "            mask_array = np.zeros((1, height, width, 1))\n",
        "            count = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    mask_array[:, h, w, :] = count\n",
        "                    count += 1\n",
        "            mask_array = tf.convert_to_tensor(mask_array)\n",
        "\n",
        "            # mask array to windows\n",
        "            mask_windows = window_partition(mask_array, self.window_size)\n",
        "            mask_windows = tf.reshape(\n",
        "                mask_windows, shape=[-1, self.window_size * self.window_size]\n",
        "            )\n",
        "            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n",
        "                mask_windows, axis=2\n",
        "            )\n",
        "            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n",
        "            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n",
        "            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n",
        "\n",
        "    def call(self, x):\n",
        "        height, width = self.num_patch\n",
        "        _, num_patches_before, channels = x.shape\n",
        "        x_skip = x\n",
        "        x = self.norm1(x)\n",
        "        x = tf.reshape(x, shape=(-1, height, width, channels))\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = tf.roll(\n",
        "                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n",
        "            )\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        x_windows = window_partition(shifted_x, self.window_size)\n",
        "        x_windows = tf.reshape(\n",
        "            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n",
        "        )\n",
        "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
        "\n",
        "        attn_windows = tf.reshape(\n",
        "            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n",
        "        )\n",
        "        shifted_x = window_reverse(\n",
        "            attn_windows, self.window_size, height, width, channels\n",
        "        )\n",
        "        if self.shift_size > 0:\n",
        "            x = tf.roll(\n",
        "                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n",
        "            )\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        x = tf.reshape(x, shape=(-1, height * width, channels))\n",
        "        x = self.drop_path(x)\n",
        "        x = x_skip + x\n",
        "        x_skip = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = self.drop_path(x)\n",
        "        x = x_skip + x\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0rtJnrMNd-i"
      },
      "source": [
        "## Model training and evaluation\n",
        "\n",
        "### Extract and embed patches\n",
        "\n",
        "We first create 3 layers to help us extract, embed and merge patches from the\n",
        "images on top of which we will later use the Swin Transformer class we built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CZ5kevzfNd-i"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PatchExtract(layers.Layer):\n",
        "    def __init__(self, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_size_x = patch_size[0]\n",
        "        self.patch_size_y = patch_size[0]\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n",
        "            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n",
        "            rates=(1, 1, 1, 1),\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dim = patches.shape[-1]\n",
        "        patch_num = patches.shape[1]\n",
        "        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
        "\n",
        "\n",
        "class PatchEmbedding(layers.Layer):\n",
        "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patch = num_patch\n",
        "        self.proj = layers.Dense(embed_dim)\n",
        "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, patch):\n",
        "        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n",
        "        return self.proj(patch) + self.pos_embed(pos)\n",
        "\n",
        "\n",
        "class PatchMerging(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_patch, embed_dim):\n",
        "        super().__init__()\n",
        "        self.num_patch = num_patch\n",
        "        self.embed_dim = embed_dim\n",
        "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n",
        "\n",
        "    def call(self, x):\n",
        "        height, width = self.num_patch\n",
        "        _, _, C = x.get_shape().as_list()\n",
        "        x = tf.reshape(x, shape=(-1, height, width, C))\n",
        "        x0 = x[:, 0::2, 0::2, :]\n",
        "        x1 = x[:, 1::2, 0::2, :]\n",
        "        x2 = x[:, 0::2, 1::2, :]\n",
        "        x3 = x[:, 1::2, 1::2, :]\n",
        "        x = tf.concat((x0, x1, x2, x3), axis=-1)\n",
        "        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n",
        "        return self.linear_trans(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xh4xsGmNd-k"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "We put together the Swin Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-GB9eMIJNd-k"
      },
      "outputs": [],
      "source": [
        "input = layers.Input(input_shape)\n",
        "x = layers.RandomCrop(image_dimension, image_dimension)(input)\n",
        "x = layers.RandomFlip(\"horizontal\")(x)\n",
        "x = PatchExtract(patch_size)(x)\n",
        "x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\n",
        "x = SwinTransformer(\n",
        "    dim=embed_dim,\n",
        "    num_patch=(num_patch_x, num_patch_y),\n",
        "    num_heads=num_heads,\n",
        "    window_size=window_size,\n",
        "    shift_size=0,\n",
        "    num_mlp=num_mlp,\n",
        "    qkv_bias=qkv_bias,\n",
        "    dropout_rate=dropout_rate,\n",
        ")(x)\n",
        "x = SwinTransformer(\n",
        "    dim=embed_dim,\n",
        "    num_patch=(num_patch_x, num_patch_y),\n",
        "    num_heads=num_heads,\n",
        "    window_size=window_size,\n",
        "    shift_size=shift_size,\n",
        "    num_mlp=num_mlp,\n",
        "    qkv_bias=qkv_bias,\n",
        "    dropout_rate=dropout_rate,\n",
        ")(x)\n",
        "x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "output = layers.Dense(num_classes, activation=\"softmax\")(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFSqdeiNNd-m"
      },
      "source": [
        "### Train on BUGS DATASET\n",
        "\n",
        "\n",
        "We train the model on CIFAR-100. Here, we only train the model\n",
        "for 40 epochs to keep the training time short in this example.\n",
        "In practice, you should train for 150 epochs to reach convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NcI3X1EhNd-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1be24e0-ebe3-423f-c6d6-73be77bf6a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "116/116 [==============================] - 130s 1s/step - loss: 32.6642 - accuracy: 0.1538 - top-5-accuracy: 0.6100 - val_loss: 20.3544 - val_accuracy: 0.1301 - val_top-5-accuracy: 0.6721\n",
            "Epoch 2/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 12.8292 - accuracy: 0.2232 - top-5-accuracy: 0.6815 - val_loss: 12.9416 - val_accuracy: 0.1978 - val_top-5-accuracy: 0.6775\n",
            "Epoch 3/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 11.5181 - accuracy: 0.2416 - top-5-accuracy: 0.6631 - val_loss: 9.5760 - val_accuracy: 0.2087 - val_top-5-accuracy: 0.7290\n",
            "Epoch 4/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 10.3858 - accuracy: 0.2730 - top-5-accuracy: 0.7107 - val_loss: 8.5083 - val_accuracy: 0.2439 - val_top-5-accuracy: 0.7724\n",
            "Epoch 5/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 5.4266 - accuracy: 0.3456 - top-5-accuracy: 0.7703 - val_loss: 4.7340 - val_accuracy: 0.3469 - val_top-5-accuracy: 0.7805\n",
            "Epoch 6/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 4.1433 - accuracy: 0.3424 - top-5-accuracy: 0.7974 - val_loss: 4.7439 - val_accuracy: 0.2981 - val_top-5-accuracy: 0.7425\n",
            "Epoch 7/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 3.3549 - accuracy: 0.3510 - top-5-accuracy: 0.8202 - val_loss: 2.4274 - val_accuracy: 0.3062 - val_top-5-accuracy: 0.8726\n",
            "Epoch 8/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 2.6030 - accuracy: 0.3965 - top-5-accuracy: 0.8548 - val_loss: 2.2439 - val_accuracy: 0.3577 - val_top-5-accuracy: 0.9187\n",
            "Epoch 9/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 2.3643 - accuracy: 0.4063 - top-5-accuracy: 0.8787 - val_loss: 2.0955 - val_accuracy: 0.4255 - val_top-5-accuracy: 0.8699\n",
            "Epoch 10/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 2.2076 - accuracy: 0.4117 - top-5-accuracy: 0.8635 - val_loss: 2.3791 - val_accuracy: 0.3957 - val_top-5-accuracy: 0.8591\n",
            "Epoch 11/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 2.1814 - accuracy: 0.4063 - top-5-accuracy: 0.8689 - val_loss: 1.9136 - val_accuracy: 0.5176 - val_top-5-accuracy: 0.9404\n",
            "Epoch 12/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.9188 - accuracy: 0.4464 - top-5-accuracy: 0.9025 - val_loss: 1.8977 - val_accuracy: 0.4390 - val_top-5-accuracy: 0.9350\n",
            "Epoch 13/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.9327 - accuracy: 0.4225 - top-5-accuracy: 0.8949 - val_loss: 1.7985 - val_accuracy: 0.3686 - val_top-5-accuracy: 0.9512\n",
            "Epoch 14/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.7280 - accuracy: 0.4518 - top-5-accuracy: 0.9350 - val_loss: 1.5796 - val_accuracy: 0.5149 - val_top-5-accuracy: 0.9593\n",
            "Epoch 15/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.7484 - accuracy: 0.4702 - top-5-accuracy: 0.9426 - val_loss: 1.5654 - val_accuracy: 0.5149 - val_top-5-accuracy: 0.9539\n",
            "Epoch 16/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.6233 - accuracy: 0.4875 - top-5-accuracy: 0.9534 - val_loss: 1.6181 - val_accuracy: 0.4797 - val_top-5-accuracy: 0.9512\n",
            "Epoch 17/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.7002 - accuracy: 0.4832 - top-5-accuracy: 0.9296 - val_loss: 1.5992 - val_accuracy: 0.4959 - val_top-5-accuracy: 0.9458\n",
            "Epoch 18/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.6659 - accuracy: 0.5027 - top-5-accuracy: 0.9469 - val_loss: 1.6488 - val_accuracy: 0.5041 - val_top-5-accuracy: 0.9675\n",
            "Epoch 19/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.5513 - accuracy: 0.5146 - top-5-accuracy: 0.9523 - val_loss: 1.5843 - val_accuracy: 0.4959 - val_top-5-accuracy: 0.9648\n",
            "Epoch 20/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.6144 - accuracy: 0.4995 - top-5-accuracy: 0.9567 - val_loss: 1.6344 - val_accuracy: 0.4553 - val_top-5-accuracy: 0.9621\n",
            "Epoch 21/40\n",
            "116/116 [==============================] - 132s 1s/step - loss: 1.5115 - accuracy: 0.5460 - top-5-accuracy: 0.9599 - val_loss: 1.5539 - val_accuracy: 0.5474 - val_top-5-accuracy: 0.9783\n",
            "Epoch 22/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.5126 - accuracy: 0.5244 - top-5-accuracy: 0.9664 - val_loss: 1.4974 - val_accuracy: 0.5691 - val_top-5-accuracy: 0.9729\n",
            "Epoch 23/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4759 - accuracy: 0.5720 - top-5-accuracy: 0.9599 - val_loss: 1.4862 - val_accuracy: 0.5501 - val_top-5-accuracy: 0.9810\n",
            "Epoch 24/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4584 - accuracy: 0.5731 - top-5-accuracy: 0.9653 - val_loss: 1.5514 - val_accuracy: 0.5827 - val_top-5-accuracy: 0.9485\n",
            "Epoch 25/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4697 - accuracy: 0.5645 - top-5-accuracy: 0.9675 - val_loss: 1.4052 - val_accuracy: 0.6016 - val_top-5-accuracy: 0.9892\n",
            "Epoch 26/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.5026 - accuracy: 0.5330 - top-5-accuracy: 0.9577 - val_loss: 1.4598 - val_accuracy: 0.5474 - val_top-5-accuracy: 0.9837\n",
            "Epoch 27/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4714 - accuracy: 0.5569 - top-5-accuracy: 0.9729 - val_loss: 1.5675 - val_accuracy: 0.4688 - val_top-5-accuracy: 0.9729\n",
            "Epoch 28/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4734 - accuracy: 0.5634 - top-5-accuracy: 0.9642 - val_loss: 1.4462 - val_accuracy: 0.5854 - val_top-5-accuracy: 0.9702\n",
            "Epoch 29/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4331 - accuracy: 0.5872 - top-5-accuracy: 0.9664 - val_loss: 1.4356 - val_accuracy: 0.6125 - val_top-5-accuracy: 0.9729\n",
            "Epoch 30/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4188 - accuracy: 0.5980 - top-5-accuracy: 0.9718 - val_loss: 1.3504 - val_accuracy: 0.6612 - val_top-5-accuracy: 0.9864\n",
            "Epoch 31/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4026 - accuracy: 0.6132 - top-5-accuracy: 0.9664 - val_loss: 1.5559 - val_accuracy: 0.5474 - val_top-5-accuracy: 0.9404\n",
            "Epoch 32/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4398 - accuracy: 0.5861 - top-5-accuracy: 0.9632 - val_loss: 1.3950 - val_accuracy: 0.6043 - val_top-5-accuracy: 0.9810\n",
            "Epoch 33/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4159 - accuracy: 0.5850 - top-5-accuracy: 0.9697 - val_loss: 1.5490 - val_accuracy: 0.4770 - val_top-5-accuracy: 0.9621\n",
            "Epoch 34/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.3938 - accuracy: 0.6089 - top-5-accuracy: 0.9740 - val_loss: 1.4931 - val_accuracy: 0.5420 - val_top-5-accuracy: 0.9756\n",
            "Epoch 35/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4131 - accuracy: 0.6089 - top-5-accuracy: 0.9686 - val_loss: 1.3433 - val_accuracy: 0.6477 - val_top-5-accuracy: 0.9892\n",
            "Epoch 36/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4343 - accuracy: 0.5959 - top-5-accuracy: 0.9675 - val_loss: 1.3665 - val_accuracy: 0.6748 - val_top-5-accuracy: 0.9810\n",
            "Epoch 37/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.3932 - accuracy: 0.6089 - top-5-accuracy: 0.9751 - val_loss: 1.3792 - val_accuracy: 0.5962 - val_top-5-accuracy: 0.9892\n",
            "Epoch 38/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.3947 - accuracy: 0.6241 - top-5-accuracy: 0.9642 - val_loss: 1.4299 - val_accuracy: 0.5989 - val_top-5-accuracy: 0.9756\n",
            "Epoch 39/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.4066 - accuracy: 0.6078 - top-5-accuracy: 0.9664 - val_loss: 1.4195 - val_accuracy: 0.5772 - val_top-5-accuracy: 0.9810\n",
            "Epoch 40/40\n",
            "116/116 [==============================] - 125s 1s/step - loss: 1.3886 - accuracy: 0.6154 - top-5-accuracy: 0.9610 - val_loss: 1.4878 - val_accuracy: 0.4905 - val_top-5-accuracy: 0.9648\n"
          ]
        }
      ],
      "source": [
        "model = keras.Model(input, output)\n",
        "model.compile(\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
        "    optimizer=tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    metrics=[\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=8, #radi na 4 - tpu. Ne radi na 8 - tpu. Nece na 16 - gpu. Oce na 8 - gpu\n",
        "    epochs=num_epochs,\n",
        "    validation_data=(x_val, y_val) #validation_split\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xRRolyRdHydr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3imYxJYdNd-n"
      },
      "source": [
        "Let's visualize the training progress of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "isAovMctNd-o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "fe298524-bbfa-47fa-8ae1-e423172b9bf3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIjoEYjZNd-o"
      },
      "source": [
        "Let's display the final results of the training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HFig2TYbNd-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddff03f0-cbf3-4ce6-c5cb-ee0bb687cd29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 19s 1s/step - loss: 1.4645 - accuracy: 0.5433 - top-5-accuracy: 0.9783\n",
            "Test loss: 1.46\n",
            "Test accuracy: 54.33%\n",
            "Test top 5 accuracy: 97.83%\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test loss: {round(loss, 2)}\")\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}